{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXdkKCj0VFcb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.read_csv('pre_EF_data.csv')"
      ],
      "metadata": {
        "id": "mYKyHm-xBFyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing RFE columns\n",
        "X_f = X.drop(columns = ['vdw_std','cov_rad_std','res_sg','z_std'],axis = 1)"
      ],
      "metadata": {
        "id": "KMDGs4u8WPeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_ef = pd.read_csv('y_EF.csv')"
      ],
      "metadata": {
        "id": "WG8mAsGnEpij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffling the dataset for better learning by model\n",
        "df_new = pd.concat([X_f, y_ef], axis=1)\n",
        "\n",
        "df_new = df_new.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "X_f = df_new.drop(columns = ['Ef'],axis = 1)\n",
        "y_ef = df_new['Ef']"
      ],
      "metadata": {
        "id": "cB1izKd_W89V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_f, y_ef, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "Hmj5V8lnZmQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X_f = StandardScaler()\n",
        "\n",
        "X_train = sc_X_f.fit_transform(X_train)\n",
        "X_test = sc_X_f.transform(X_test)"
      ],
      "metadata": {
        "id": "0-lp35lzZuW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Keras Tuner for EF model**"
      ],
      "metadata": {
        "id": "Apx4gmimjutm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf"
      ],
      "metadata": {
        "id": "ymF8DGfm_-4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras_tuner"
      ],
      "metadata": {
        "id": "XrGi99zHbA81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "# from tensorflow.keras.layers import Dropout\n",
        "\n",
        "\n",
        "# import keras_tuner as kt\n",
        "# def build_model_ef(hp):\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # Adding the input layer\n",
        "#     model.add(Dense(\n",
        "#         units=hp.Int('units_input', min_value=150, max_value=200, step=50),\n",
        "#         activation=hp.Choice('activation0',values = ['relu','tanh'])\n",
        "#         input_dim=X_train.shape[1]\n",
        "#     ))\n",
        "\n",
        "#     model.add(Dropout(rate=hp.Float('dropout_input', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "#     # Adding intermediate layers\n",
        "#     for i in range(hp.Int('num_layers', min_value=1, max_value=5)):\n",
        "#         model.add(Dense(\n",
        "#             units=hp.Int(f'units_{i}', min_value=75, max_value=150, step=25),\n",
        "#             activation=hp.Choice(f'activation_{i}',values = ['relu','tanh'])\n",
        "#         ))\n",
        "#         model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "#     # Adding the output layer\n",
        "#     model.add(Dense(units=1))\n",
        "\n",
        "#     model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "\n",
        "#     return model\n",
        "\n",
        "# tuner_ef_arch = kt.RandomSearch(build_model_ef, objective='val_loss', max_trials=40, directory='mydir', project_name='ef_model')\n"
      ],
      "metadata": {
        "id": "s861g8fgaHEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuner_ef_arch.search(X_train,y_train,batch_size = 32,epochs = 50 ,validation_data = (X_test , y_test))\n",
        "# tuner_ef_arch.get_best_hyperparameters()[0].values"
      ],
      "metadata": {
        "id": "ROWfWD4GaWqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuner_ef_arch.get_best_hyperparameters()[0].values"
      ],
      "metadata": {
        "id": "-wmTrduIV3Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def build_model_ef_lr(hp):\n",
        "#   model = tf.keras.models.Sequential()\n",
        "#   model.add(tf.keras.layers.Dense(units=200, input_dim=X_train.shape[1], activation='relu'))\n",
        "#   model.add(tf.keras.layers.Dropout(0.15))\n",
        "#   model.add(tf.keras.layers.Dense(units=150, activation='relu'))\n",
        "#   model.add(tf.keras.layers.Dropout(0.2))\n",
        "#   model.add(tf.keras.layers.Dense(units=75, activation='tanh'))\n",
        "#   model.add(tf.keras.layers.Dropout(0.1))\n",
        "#   model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "#   epoch = 150\n",
        "#   batch_size = 32\n",
        "\n",
        "#   initial_learning_rate = hp.Float('learning_rate',min_value = 1e-5,max_value = 1e-2,step = 1e-5)\n",
        "\n",
        "#   optimizer = tf.keras.optimizers.Adam(\n",
        "#     learning_rate = initial_learning_rate)\n",
        "\n",
        "#   model.compile(loss='mean_squared_error',optimizer = optimizer, metrics='mean_squared_error')\n",
        "\n",
        "#   return model\n",
        "\n",
        "# tuner_ef_lr = kt.RandomSearch(build_model_ef_lr, objective='val_loss', max_trials=40, directory='mydir', project_name='ef_lr')\n",
        "# tuner_ef_lr.search(X_train,y_train,batch_size = 32,epochs = 50 ,validation_data = (X_test , y_test))\n",
        "# tuner_ef_lr.get_best_hyperparameters()[0].values\n"
      ],
      "metadata": {
        "id": "bNh3Hk5maDQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ef Model with optimized parameters after kears tuner**"
      ],
      "metadata": {
        "id": "wKIqHvjMj95L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=200, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.15))\n",
        "model.add(tf.keras.layers.Dense(units=150, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(units=75, activation='tanh'))\n",
        "model.add(tf.keras.layers.Dropout(0.1))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "epochs = 150\n",
        "batch_size = 32\n",
        "\n",
        "initial_learning_rate = 0.0011 #As per the keras tuner\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate = initial_learning_rate)"
      ],
      "metadata": {
        "id": "Nmxna6n_Tww1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "class LossHistory(Callback):\n",
        "    def __init__(self):\n",
        "        self.val_losses = []\n",
        "        self.losses = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.losses.append(logs.get('loss'))\n",
        "history = LossHistory()"
      ],
      "metadata": {
        "id": "zRfGy71bX31i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=40,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "Ge-mg5KMV5Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "filepath = '/content/model_ef.h5'\n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')"
      ],
      "metadata": {
        "id": "o7SUpIjXcEBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann.compile(loss='mean_squared_error',optimizer = optimizer, metrics='mean_squared_error')"
      ],
      "metadata": {
        "id": "khQ_g62QWRAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann.fit(X_train, y_train, validation_data = (X_test,y_test) , batch_size = 32, epochs = 150,callbacks = [checkpoint,history,early_stopping])"
      ],
      "metadata": {
        "id": "Wfw4iJtvWX1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/model_ef.h5')"
      ],
      "metadata": {
        "id": "9IYf_jGacZDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score , mean_absolute_error\n",
        "import numpy as np\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mean_abs = mean_absolute_error(y_test,y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "print(\"R-squared Score:\", r2)\n",
        "print(\"Mean Absolute Error:\", mean_abs)"
      ],
      "metadata": {
        "id": "vwM6L91gYLj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_losses = history.val_losses\n",
        "loss = history.losses"
      ],
      "metadata": {
        "id": "IE-poQaBYNno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = list(range(1, 151))\n",
        "\n",
        "plt.plot(epochs, val_losses, label='Val_Loss')\n",
        "plt.plot(epochs, loss, label='Training Loss')\n",
        "plt.title('Epoch vs Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Iw0jvNvxYTLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using predicted values of Ef by the model  , to augment into EG database\n",
        "ef = model.predict(X_f)"
      ],
      "metadata": {
        "id": "XH21XdcvyZhQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using K-Fold with optimized model hyperparameters**"
      ],
      "metadata": {
        "id": "mvE89M1-hMMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "errors = []\n",
        "kernels = []\n",
        "\n",
        "for idx, (train, val) in enumerate(kf.split(X_f)):\n",
        "    _X_cv_train = X_f.values[train]\n",
        "    _X_cv_val = X_f.values[val]\n",
        "\n",
        "    X_cv_train = scaler.fit_transform(_X_cv_train)\n",
        "    X_cv_val = scaler.transform(_X_cv_val)\n",
        "\n",
        "    y_cv_train = y_ef.values[train]\n",
        "    y_cv_val = y_ef.values[val]\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(units=200, input_dim=X_cv_train.shape[1], activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.15))\n",
        "    model.add(tf.keras.layers.Dense(units=150, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(units=75, activation='tanh'))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "    epochs = 150\n",
        "    batch_size = 32\n",
        "\n",
        "    initial_learning_rate = 0.0011\n",
        "\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate = initial_learning_rate)\n",
        "\n",
        "    model.compile(loss='mean_squared_error',optimizer = optimizer, metrics='mean_squared_error')\n",
        "    model.fit(X_cv_train, y_cv_train, validation_data = (X_cv_val,y_cv_val) , batch_size = 32, epochs = 150,callbacks = [checkpoint])\n",
        "\n",
        "    model = load_model('/content/model_ef.h5')\n",
        "\n",
        "    y_pred_val = model.predict(X_cv_val)\n",
        "    y_pred_train = model.predict(X_cv_train)\n",
        "\n",
        "    # Computing errors\n",
        "    rmse_val = np.sqrt(mean_squared_error(y_cv_val, y_pred_val))\n",
        "    mse_val = mean_squared_error(y_cv_val, y_pred_val)\n",
        "    rmse_train = np.sqrt(mean_squared_error(y_cv_train, y_pred_train))\n",
        "\n",
        "    r2_val = r2_score(y_cv_val, y_pred_val)\n",
        "    r2_train = r2_score(y_cv_train, y_pred_train)\n",
        "\n",
        "    print(\"Root mean squared error: %.2f\" % rmse_val)\n",
        "    print(\"Coefficient of determination: %.2f\" % r2_val)\n",
        "\n",
        "    error = {'kfold': idx+1,\n",
        "             'rmse_train': rmse_train,\n",
        "             'r2_train': r2_train,\n",
        "             'rmse_val': rmse_val,\n",
        "             'r2_val': r2_val,\n",
        "             'mse_val': mse_val\n",
        "            }\n",
        "\n",
        "    errors.append(error)"
      ],
      "metadata": {
        "id": "LHIvLIuehW9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors_gpr_cv_opt_alpha = pd.DataFrame(errors)\n",
        "errors_gpr_cv_opt_alpha.to_csv('ef_error_cv.csv',index = False)"
      ],
      "metadata": {
        "id": "TUV_kyIDhzkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_means = errors_gpr_cv_opt_alpha.mean()\n",
        "print(column_means)"
      ],
      "metadata": {
        "id": "c6l170P_iJne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EG Model**"
      ],
      "metadata": {
        "id": "ct1UrpaNiB2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing RFE columns\n",
        "X_g = pd.DataFrame(X.drop(columns =['spec_heat_mean','cs1','atom_mass_mean'],axis = 1))\n",
        "X_g['Ef'] = ef"
      ],
      "metadata": {
        "id": "dTkGaaue7f3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_eg = pd.read_csv('y_Eg.csv')"
      ],
      "metadata": {
        "id": "vs04cEgoh5i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_g, y_eg, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "4SFVzhJ65knM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X_g = StandardScaler()\n",
        "\n",
        "X_train = sc_X_g.fit_transform(X_train)\n",
        "X_test = sc_X_g.transform(X_test)"
      ],
      "metadata": {
        "id": "Bp6HADgl7lz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Keras Tuner for EG model**"
      ],
      "metadata": {
        "id": "crZ7hX28jOvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def build_model_eg(hp):\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # Adding the input layer\n",
        "#     model.add(Dense(\n",
        "#         units=hp.Int('units_input', min_value=150, max_value=200, step=50),\n",
        "#         activation=hp.Choice('activation0',values = ['relu','tanh'])\n",
        "#         input_dim=X_train.shape[1]\n",
        "#     ))\n",
        "\n",
        "#     model.add(Dropout(rate=hp.Float('dropout_input', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "#     # Adding intermediate layers\n",
        "#     for i in range(hp.Int('num_layers', min_value=1, max_value=5)):\n",
        "#         model.add(Dense(\n",
        "#             units=hp.Int(f'units_{i}', min_value=75, max_value=150, step=25),\n",
        "#             activation=hp.Choice(f'activation_{i}',values = ['relu','tanh'])\n",
        "#         ))\n",
        "#         model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "#     # Adding the output layer\n",
        "#     model.add(Dense(units=1))\n",
        "\n",
        "#     model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "\n",
        "#     return model\n",
        "\n",
        "# tuner_eg_arch = kt.RandomSearch(build_model_eg, objective='val_loss', max_trials=50, directory='mydir', project_name='eg_model')\n"
      ],
      "metadata": {
        "id": "xrVaubX4-ErM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuner_eg_arch.search(X_train,y_train,batch_size = 32,epochs = 50 ,validation_data = (X_test , y_test))\n",
        "# tuner_eg_arch.get_best_hyperparameters()[0].values"
      ],
      "metadata": {
        "id": "-WqK3WgoacLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuner_eg_arch.get_best_hyperparameters()[0].values"
      ],
      "metadata": {
        "id": "IR9bDiI8afW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def build_model_eg_lr(hp):\n",
        "  # model = tf.keras.models.Sequential()\n",
        "  # model.add(tf.keras.layers.Dense(units=200, input_dim=X.shape[1], activation='tanh'))\n",
        "  # model.add(tf.keras.layers.Dropout(0.1))\n",
        "  # model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n",
        "  # model.add(tf.keras.layers.Dropout(0.2))\n",
        "  # model.add(tf.keras.layers.Dense(units=100, activation='tanh'))\n",
        "  # model.add(tf.keras.layers.Dropout(0.2))\n",
        "  # model.add(tf.keras.layers.Dense(units=125, activation='tanh'))\n",
        "  # model.add(tf.keras.layers.Dropout(0.5))\n",
        "  # model.add(tf.keras.layers.Dense(units=150, activation='tanh'))\n",
        "  # model.add(tf.keras.layers.Dropout(0.1))\n",
        "  # model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "#   epoch = 150\n",
        "#   batch_size = 32\n",
        "\n",
        "#   initial_learning_rate = hp.Float('learning_rate',min_value = 1e-5,max_value = 1e-2,step = 1e-5)\n",
        "\n",
        "#   optimizer = tf.keras.optimizers.Adam(\n",
        "#     learning_rate = initial_learning_rate)\n",
        "\n",
        "#   model.compile(loss='mean_squared_error',optimizer = optimizer, metrics='mean_squared_error')\n",
        "\n",
        "#   return model\n",
        "\n",
        "# tuner_eg_lr = kt.RandomSearch(build_model_eg_lr, objective='val_loss', max_trials=40, directory='mydir', project_name='eg_lr')\n",
        "# tuner_eg_lr.search(X_train,y_train,batch_size = 32,epochs = 50 ,validation_data = (X_test , y_test))\n",
        "# tuner_eg_lr.get_best_hyperparameters()[0].values\n"
      ],
      "metadata": {
        "id": "flvA94-ua_kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Eg Model with optimized parameters after kears tuner**"
      ],
      "metadata": {
        "id": "wyV57aQrjaWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=200, input_dim=X.shape[1], activation='tanh'))\n",
        "model.add(tf.keras.layers.Dropout(0.1))\n",
        "model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(units=100, activation='tanh'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(units=125, activation='tanh'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(units=150, activation='tanh'))\n",
        "model.add(tf.keras.layers.Dropout(0.1))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "epochs = 150\n",
        "batch_size = 32\n",
        "\n",
        "initial_learning_rate = 0.0011\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate = initial_learning_rate)"
      ],
      "metadata": {
        "id": "UZP8I8uP6bQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/model_eg.h5'\n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')"
      ],
      "metadata": {
        "id": "oRH3kJWPsvv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error',optimizer = optimizer, metrics='mean_squared_error')"
      ],
      "metadata": {
        "id": "2n7ND8626exO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, validation_data = (X_test,y_test) , batch_size = batch_size, epochs = epochs,callbacks = [early_stopping,checkpoint,history])"
      ],
      "metadata": {
        "id": "FaTwGVeE6iJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/model_eg.h5')"
      ],
      "metadata": {
        "id": "Mvwe2fSUs1F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "RGV4wgdB6nDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mean_abs = mean_absolute_error(y_test,y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "print(\"R-squared Score:\", r2)\n",
        "print(\"Mean Absolute Error:\", mean_abs)"
      ],
      "metadata": {
        "id": "_8BdqOMoWRoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_losses = history.val_losses\n",
        "loss = history.losses"
      ],
      "metadata": {
        "id": "P1t_2AAIc5ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = list(range(1, 151))\n",
        "\n",
        "plt.plot(epochs, val_losses, label='Val_Loss')\n",
        "plt.plot(epochs, loss, label='Training Loss')\n",
        "plt.title('Epoch vs Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GqN-5KONc52K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using KFold**"
      ],
      "metadata": {
        "id": "6K0IcZs1irBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "errors = []\n",
        "kernels = []\n",
        "\n",
        "for idx, (train, val) in enumerate(kf.split(X_g)):\n",
        "    _X_cv_train = X_g.values[train]\n",
        "    _X_cv_val = X_g.values[val]\n",
        "\n",
        "    X_cv_train = scaler.fit_transform(_X_cv_train)\n",
        "    X_cv_val = scaler.transform(_X_cv_val)\n",
        "\n",
        "    y_cv_train = y_eg.values[train]\n",
        "    y_cv_val = y_eg.values[val]\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(units=200, input_dim=X_cv_train.shape[1], activation='tanh'))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(units=100, activation='tanh'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(units=125, activation='tanh'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(units=150, activation='tanh'))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "    epochs = 150\n",
        "    batch_size = 32\n",
        "    eval_batch_size = 32\n",
        "\n",
        "    train_data_size = X_cv_train.shape[0]\n",
        "    steps_per_epoch = int(train_data_size / batch_size)\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    warmup_steps = int(0.25* num_train_steps)\n",
        "    initial_learning_rate = 0.0011\n",
        "\n",
        "    linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        end_learning_rate=1e-4,\n",
        "        decay_steps=num_train_steps)\n",
        "\n",
        "    warmup_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        decay_steps=warmup_steps,\n",
        "        end_learning_rate=initial_learning_rate,\n",
        "        power=1.25\n",
        "    )\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate = warmup_schedule)\n",
        "\n",
        "    model.compile(loss='mean_squared_error',optimizer = optimizer, metrics='mean_squared_error')\n",
        "    model.fit(X_cv_train, y_cv_train, validation_data = (X_cv_val,y_cv_val) , batch_size = 32, epochs = 150,callbacks = [checkpoint])\n",
        "\n",
        "    model = load_model('/content/model_eg.h5')\n",
        "\n",
        "    y_pred_val = model.predict(X_cv_val)\n",
        "    y_pred_train = model.predict(X_cv_train)\n",
        "\n",
        "    # Computing errors\n",
        "    rmse_val = np.sqrt(mean_squared_error(y_cv_val, y_pred_val))\n",
        "    mse_val = mean_squared_error(y_cv_val, y_pred_val)\n",
        "    rmse_train = np.sqrt(mean_squared_error(y_cv_train, y_pred_train))\n",
        "\n",
        "    r2_val = r2_score(y_cv_val, y_pred_val)\n",
        "    r2_train = r2_score(y_cv_train, y_pred_train)\n",
        "\n",
        "    print(\"Root mean squared error: %.2f\" % rmse_val)\n",
        "    print(\"Coefficient of determination: %.2f\" % r2_val)\n",
        "\n",
        "    error = {'kfold': idx+1,\n",
        "             'rmse_train': rmse_train,\n",
        "             'r2_train': r2_train,\n",
        "             'rmse_val': rmse_val,\n",
        "             'r2_val': r2_val,\n",
        "             'mse_val': mse_val\n",
        "            }\n",
        "\n",
        "    errors.append(error)"
      ],
      "metadata": {
        "id": "lNCSriDHih4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors_gpr_cv_opt_alpha = pd.DataFrame(errors)\n",
        "errors_gpr_cv_opt_alpha.to_csv('eg_cv.csv',index = False)"
      ],
      "metadata": {
        "id": "_AXjGdNUjEk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_means = errors_gpr_cv_opt_alpha.mean()\n",
        "print(column_means)"
      ],
      "metadata": {
        "id": "9_nenQe4jIiD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}